reticulate::repl_python()
5+5
quit
r2d3::r2d3("Users/joshj/Documents/d3.js", data=c(0.3, 0.6, 0.8, 0.95, 0.40, 0.20))
setwd("C:/byu_masters/hartman_gan/Auto_GAN/auto_gan")
reticulate::repl_python()
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.autograd.variable import Variable
from tqdm import tqdm
from gan_scripts.auto_loader import PolicyDataset
from gan_scripts.autoencoder import AutoEncoder, autoencoder_loss
from gan_scripts.generator import Generator
from gan_scripts.discriminator import Discriminator
# import modules
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.autograd.variable import Variable
from tqdm import tqdm
import pandas as pd
import numpy as np
import statsmodels.api as sm
from patsy import dmatrices
pd.set_option('display.float_format', lambda x: '%.2f' % x)
## Import created modules
from gan_scripts.auto_loader import PolicyDataset
from gan_scripts.generator2_v2 import Generator2
from gan_scripts.discriminator2_v3 import Discriminator2
from gan_scripts.gradiant_penalty import calculate_gradient_penalty
from gan_scripts.undo_dummy import back_from_dummies
# Load and edit data
policy1 = pd.read_csv("./datasets/policy_dat_v2.csv")
policy1['ClaimNb'] = policy1['ClaimNb'].astype('category')
policy1['ExposureCat'] = policy1['Exposure_cat'].astype('category')
policy1['DensityCat'] = policy1['Density_cat'].astype('category')
policy_cat = pd.get_dummies(policy1.loc[:,["ClaimNb",
"Power",
"Brand",
"Gas",
"Region",
"ExposureCat",
"DensityCat"]])
cont_vars = policy1[['CarAge','DriverAge']]
# Center and scale continuous variables
cont_vars2 = (cont_vars - np.mean(cont_vars))/np.std(cont_vars)
pol_dat = pd.concat([cont_vars2.reset_index(drop=True), policy_cat], axis=1)
# Take a sampel of the data for quickly training
pol_dat  = pol_dat.sample(n = 10000, random_state = 12)
# Fit a poisson regression model
td = back_from_dummies(pol_dat)
td['ClaimNb'] = td['ClaimNb'].astype('int')
y_real, X_real = dmatrices('ClaimNb ~ CarAge + DriverAge + Power + Brand + Gas + Region + DensityCat',
data=td,
return_type='dataframe')
td['Exposure'] = td['ExposureCat'].astype('float32')/11
def xpxixpy(X,y):
return np.dot(np.linalg.inv(np.dot(X.T,X)), np.dot(X.T,y))
xy = xpxixpy(X_real,y_real)
disc_add_rows = xy.shape[0]
poisson_mod = sm.GLM(y_real,X_real,family = sm.families.Poisson(), offset = td['Exposure']).fit()
original_params = poisson_mod.params
lower = poisson_mod.params - 1.96*poisson_mod.bse
upper = poisson_mod.params + 1.96*poisson_mod.bse
"""
This next section contains everything that we can tune in the GAN
"""
# Information about the size of the data
data_size = pol_dat.shape[1] # number of cols in pol_dat
var_locs = [0,1] # tells us where the continous variables are
# parameters
z_size = 100 # how big is the random vector fed into the generator
# we should only need the 55?
batch_size = 250
temperature = None # comes into play with the categorical activation see multioutput.py
# Generator tuning
gen_hidden_sizes = [100,100,100]
gen_bn_decay = .25
gen_l2_regularization = 0.1
gen_learning_rate = 0.001
noise_size = z_size
output_size = [1,1,5,12,7,2,10,12,5]  # how many categories with in each variable
# Discriminator tuning
disc_hidden_sizes = [55,55]
disc_bn_decay = .2
critic_bool = True # if false then between 0 and 1
mini_batch_bool = True
disc_leaky_param = 0.2
disc_l2_regularization = 0.0
disc_learning_rate = 0.001
penalty = 1 ## deals with gradiant penalty
auto_data = PolicyDataset(pol_dat, var_locs)
auto_loader = DataLoader(auto_data,
batch_size = batch_size,
pin_memory = True,
shuffle = True
)
# initilize generator and discriminator
generator = Generator2(
noise_size = noise_size,
output_size =  output_size,
hidden_sizes = gen_hidden_sizes,
bn_decay = gen_bn_decay
)
discriminator = Discriminator2(
input_size = data_size,
hidden_sizes= disc_hidden_sizes,
bn_decay = disc_bn_decay,               # no batch normalization for the critic
critic = critic_bool,                   # Do you want a critic
leaky_param = disc_leaky_param,         # parameter for leakyRelu
mini_batch = mini_batch_bool,           # Do you want any mini batch extras
add_rows = disc_add_rows # Number of rows to add if appending extra rows
)
optim_gen = optim.Adam(generator.parameters(),
weight_decay= gen_l2_regularization,
lr= gen_learning_rate
)
optim_disc = optim.Adam(discriminator.parameters(),
weight_decay= disc_l2_regularization,
lr= disc_learning_rate
)
# option to load pretrained parameters. Make sure all of the parameters of the pretrained
# Model match up with the parameters you inilized above
#generator.load_state_dict(torch.load('./training_iterations/test_start/generator_4_4050_50'))
#optim_gen.load_state_dict(torch.load('./training_iterations/test_start/gen_optim_3_3750_56'))
#discriminator.load_state_dict(torch.load('./training_iterations/test_start/discriminator_3_3850_16'))
#optim_disc.load_state_dict(torch.load('./training_iterations/test_start/disc_optim_3_3850_16'))
epochs = 1000
disc_epochs = 2
gen_epochs = 1
generator.train(mode=True)
discriminator.train(mode=True)
disc_losses = []
gen_losses = []
disc_loss = torch.tensor(9999)
gen_loss = torch.tensor(9999)
loop = tqdm(total = epochs, position = 0, leave = False)
for epoch in range(epochs):
for d_epoch in range(disc_epochs):
for c1,c2 in auto_loader: # c1 is continous variables and c2 is the categorical variables
batch = torch.cat([c2,c1],1)
optim_disc.zero_grad()
# train discriminator with real data
real_features = Variable(batch)
real_pred = discriminator(real_features)
# the disc outputs high numbers if it thinks the data is real, we take the negative of this
# Because we are minimizing loss
real_loss = -real_pred.mean(0).view(1)
real_loss.backward()
